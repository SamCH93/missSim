---
title: "Handling Missingness and Non-Convergence in Simulation Studies"
subtitle: "Randomizing papers for agreement assessment"
author: 
 - Samuel Pawel
 - Björn S. Siepe
 - Anna Lohmann
 - František Bartoš
date: "15 August 2024"
format:
  html:
    toc: true
    number-sections: true
    theme: cosmo
    code-fold: true
    code-tools: true
    code-summary: "Show the code"
    fig-width: 7
    fig-height: 4.5
execute:
  message: false
  warning: false
---

# Background
This document contains code to randomize papers from the literature review to assess agreement between different raters.

# Prep

Load the data and the tidyverse packages: 
```{r}
set.seed(2024)
library(tidyverse)
library(readxl)

# Load the data
data_fb <- read_xlsx("data/lit_review_data_FB.xlsx")
data_sp <- read_xlsx("data/lit_review_data_SP.xlsx")
data_al <- read_xlsx("data/lit_review_data_AL.xlsx")
data_bs <- read_xlsx("data/lit_review_data_BS.xlsx")

```

Combine data sets: 
```{r}
data_review <- rbind(data_fb, data_sp, data_al, data_bs)

# delete all rows without doi
data_review <- data_review[!is.na(data_review$doi), ]

# delete econ papers
data_review <- data_review |> 
  filter(journal != "EM")


```




# Randomize papers

As we preregistered:

> Fifty studies will be coded in duplicate by a second coder. We will start with the studies that have received the lowest confidence rating (and randomly break ties). 

```{r}
# Names of coders
coders <- unique(data_review$reviewer)

# start with low certainty
low_cert <- data_review |> 
  filter(Q7_coding_confidence == "low")

# count number of low certainty rows per reviewer
low_cert_num <- low_cert |> 
  count(reviewer)


# get additional rows with "medium" certainty
rows_needed <- 50 - nrow(low_cert)
medium_cert <- data_review |> 
  filter(Q7_coding_confidence == "medium")
  
# stratified sampling to balance whose papers are reviewed
# calculate rows to sample for each reviewer
n_reviewers <- length(coders)
  
# stratified sample
# (yes this is inefficient but fine here)
stratified_sample <- data.frame()
  
# loop over each reviewer
# consider existing low certainty rows
rows_per_reviewer <- setNames(rep(0, length(coders)), coders)
  
for (coder in coders) {
  # calculate rows needed for this coder 
   existing_rows <- low_cert_num |> 
      filter(reviewer == coder) |> 
      pull(n)
   
  # at most 12 to review
  rows_per_reviewer[coder] <- max(12 - existing_rows, 0)
  
  # if no papers needed, go to next reviewer
  if(rows_per_reviewer[coder] == 0) {
    next
  }
    
  coder_medium <- medium_cert |> 
    filter(reviewer == coder)
    
  # if there are fewer rows than needed, take all
  if (nrow(coder_rows) <= rows_per_reviewer[coder]) {
      stratified_sample <- rbind(stratified_sample, coder_medium)
    # otherwise sample
    } else {
      stratified_sample <- rbind(stratified_sample, coder_medium[sample(1:nrow(coder_medium), rows_per_reviewer[coder]), ])
    }
} # end loop across coders

  
# additional rows needed? sample randomly
extra_rows_needed <- rows_needed - nrow(stratified_sample)
if (extra_rows_needed > 0) {
    additional_sample <- medium_cert |>
      filter(!doi %in% stratified_sample$doi) |>
      slice_sample(n = extra_rows_needed)
    stratified_sample <- rbind(stratified_sample, additional_sample)
}

# combine low and medium certainty
final_sample <- rbind(low_cert, stratified_sample)

# if too many rows, delete one paper by BS (has the highest count)
if (extra_rows_needed < 0) {
  # delete one paper with reviewer == BS
  doi_delete <- final_sample |>
    filter(reviewer == "BS") |>
    slice_sample(n = abs(extra_rows_needed)) |>
    pull(doi)
  final_sample <- final_sample |>
    filter(!doi %in% doi_delete)
}


# randomize the new coder in a new column until all coders either have 12 or 13 papers
nc_check <- FALSE

while(isFALSE(nc_check)){
  final_sample$new_coder <- sapply(final_sample$reviewer, function(x) {
    sample(setdiff(coders, x), 1)
  })
  
  nc_check <- all(final_sample %>%
                    count(new_coder) %>%
                    pull(n) %in% c(12, 13))
}
```

Check the number of papers from each reviewer:
```{r}
final_sample %>%
  count(reviewer)
```

Check the number of papers each coder has to code:
```{r}
final_sample |> 
  count(new_coder)
```


Create a list of DOIs for each individual coder and save to disk:
```{r}
final_sample |> 
  select(new_coder, doi, journal, issue) |> 
  arrange(new_coder) |> 
  write_csv2("literature-review/coding-agreement/agreement_dois_per_coder.csv")
```


# Correction of Assignment

Due to a small bug, there was an error in coder assignment: BS should not code papers 
https://doi.org/10.1080/01621459.2021.1923511 (duplicate with SP) and https://doi.org/10.1037/met0000416 (reviewed it himself). 

```{r}
# reread the dois
dois_per_coder <- read_csv2("literature-review/coding-agreement/agreement_dois_per_coder.csv")

# remove the two dois
dois_per_coder_new <- dois_per_coder |>
  filter(doi != "https://doi.org/10.1080/01621459.2021.1923511") |> 
  filter(doi != "https://doi.org/10.1037/met0000416")


# randomly draw two new papers for BS that have not been used yet
dois_per_coder_clean <- data_review |> 
  filter(doi != "10.1080/01621459.2021.1923511" & doi != "10.1037/met0000416") |> 
  filter(reviewer != "BS") |>
  filter(!doi %in% dois_per_coder$doi) |> 
  filter(Q7_coding_confidence == "medium") |> 
  slice_sample(n = 2) |>
  mutate(new_coder = "BS") |> 
  select(new_coder, doi, journal, issue, reviewer) |>  
  bind_rows(dois_per_coder_new)

# some sanity checks
dois_per_coder_clean |> 
  count(doi) |> 
  filter(n > 1)

dois_per_coder_clean |> 
  filter(reviewer == new_coder)

# save to disk
dois_per_coder_clean |> 
  write_csv2("literature-review/coding-agreement/agreement_dois_per_coder_new.csv")

```



